* Overview

lexin reads a specification file and generates the C++ source code for
one or more functions that use regular expressions to parse strings.
Its target application:
    a) is extracting information from strings.
    b) knows the patterns it must match at compile time.
    c) is complex enough that using regular expressions becomes an unreadable mess.
    d) desires more granula error detection than monolithic regular expressions
       can provide.

It differs from more common approaches by allowing the user to break
their regular expressions up into grammars. This syntax is much easier to
implement correctly, read, and understand than putting all that complexity
into some flavor of regular expression syntax.

** default actions vis a vis lex
The lex default action "consists of copying the input to the output."
[http://dinosaur.compilertools.net/lex/index.html]
Keep in mind that we are producing a simple regular grammar parser not
just a lexical analyzer, and we expect to have been passed the entire
input as a string.

Because lexin is an integrated parser/lexer, we don't need the
flexibility/complexity of associating actions with each regular expression.
Almost. That still leaves the common need to easily let the parser
ignore tokens globally or within some context. This invariably is
the specific need to ignore or not ignore some form of white space.

We also would rather not have to have actions for both the lexer
and the parser.

Suppose we have this definition:

WHITESPACE   [ \t\r\n\v\f]

Then how do we specify that the parser should ignore WHITESPACE,
either globally or within some production? If there is a syntax for
ignoring within some production, then the two cases are the same --
just signal what you want to ignore in the top-most production
to be "global." Something like:

all
    : request-line SP request-target SP HTTP-version CRLF
    ! WHITESPACE
    ;

How is this implemented? Suppose each regex has an associated
token ID, in the customary manner. Maintain an array of integers,
one for each token ID, intialized to zero. When we enter the
state for "all", we increment:
    ++IgnoreTokens[WHITESPACE_ID];
When we exit (conceptually, at least) the state for "all", we decrement:
    --IgnoreTokens[WHITESPACE_ID];

During the parse, when we get an unexpected token, if the corresponding
entry in IgnoreTokens is greater than zero, we fetch a new token and
return to the same parser state.

When generating DFA code for the parser, each state corresponding
to a production must start by pushing the current input position
onto the (fixed-size) position stack. When generating code for
the final state of a production, the code will note the total length,
adjust the stack pointer down, then invoke any user action.

** the parser DFA

We conceptually want control to "enter" a non-terminal when
it starts and "exit" it when it matches or fails to match.
A regular grammar parser needs no stack, but we do because
we want to "remember" the start position of the substring
that the production matches. However, we can calculate at
compile-time the maximum stack needed because a regular grammar
allows no recursion.

Note that we don't allow you to attach an action to sub-productions.
Consider this:

statement-list
    : ( "$" statement "." ) *
    ;


** actions





** regular expressions and grammars

x
N1 : 'x'
   ;
xy
N1 : 'x' 'y'
   ;
x|y
N1 : 'x' | 'y'
   ;

x*
N1: x N1
  | 
  ;



** An Incremental Example
Suppose we wish to scan text for a telephone number. Here is a simple lexin
specification for that task:

// example1.xin - find telephone numbers
%%
phnumber
    : "(" areacode ")" prefix "-" number
        {
        printf("Found phone number: (%.*s) %.*s-%.*s\n",
            $areacode.len, $areacode,
            $prefix.len, $prefix,
            $number.len, $number);
        }
    ;
areacode : '[0-9]{3}' ;

prefix   : '[0-9]{3}' ;
number   : '[0-9]{4}' ;

Here we see the lexin syntax, which looks like an amalgam of the traditional
compiler tools lex and yacc. Although it looks like a standard BNF syntax of
grammar is accepted, lexin only generates simple state machines and will complain
if asked to parse anything more complicated than a regular grammar
(i.e., a grammar that is equivalent to some regular expression).

The quoted constants in this example are regular expressions. As with lex,
double quotes are used to enclose literal strings (which are also still regular
expressions). So '[0-9]' is a regular expression matching any ASCII digit, while
"[0-9]" is a regular expression matching the unlikely 5-character sequence:
"[0-9]".



* Design

** Overview

Sub-expressions are hard to write, understand, and implement. Lexin's
approach is to instead treat sub-expressions as separate expressions,
in which the familiar "longest match" behavior can be assumed.

Consider Stack Overflow question:
 https://stackoverflow.com/questions/42094483/regex-sub-expressions

That begins to be fairly hard to understand and get right. The
more verbose (and easier to read and verify) lexin solution might be:

argchr
    : [^:]
    | "\\:"
    ;
arg : argchr* ;
repl
    : "replace:" arg ":" arg
        PRINT($2, $4);
    ;

Note that this is NOT the same as:
    "replace:([^:]|\\:)*:([^:]|\\:)*"

** syntax
The overall file syntax goes like this:
    definitions
    %%
    grammar

*** definitions
The initial section of the lexin file is for creating names for
regular definitions that will be used later. This is never strictly
necessary, but is often convenient in line with the goal of making
the use of regular expressions readable and understandable.


*** grammar
The grammar section consists of two types of entities: 


** Typical Problems
*** custom scan fragment

*** matching "words"
Regular expression software often offers an operator like "\w" to
match something like "[0-9a-zA-Z_]"


*** nested parens
This is, of course, beyond the ability of a regular expression,
which "cannot count."

*** C-style comments
The first thought is something like:
    "/*".*"*/"
But because of the longest-match convention, that matches, e.g., this:
    /* comment1 */ /* but I will match until LAST closer */
If we had intersect and negate operators:
    "/*"(.*&!.*"*/")"*/"
But look how unreadable that begins to get already. What if "/" is
the "up to" operator and works on the next regdef:

c_comment
    : "/*" ~ "*/"
        { $$ = $2; } // return the content of the comment

This means a) first maximally match "/*" then b) mark pos and do
unanchored search for "*/" then c) back up to start of located pattern
and deliver pos thru curpos as result of "/" then deliver remainder
as match for "*/".

*** XML tags

Similar to the C comment problem, the first thought might be:
    <.*>
and the problem is that this will match this entire string:
    <start1> and some stuff <start2>
So the solution is similar:
    '<' ~ '>'

Now, can we do this:

tag: stag | etag ;
stag
    : "<" ~ ">"
    ;
etag
    : "<" ~ "/>"
    ;

What does the parse DFA for this look like? Maybe this:
    // 0 = start state
    0 : "<" -> 1
    // 1 = unanchored scan for ">" or "/>"
    1 : "[^/>]" -> 1
      : "/"     -> 2
      : ">"     -> 4
    // 2 = got "/", check for ">"
    2 : ">"     -> 3
      : "/"     -> 2
      : .       -> 1
    // 3 = got "/>"
    3 :
    // 4 = got ">"
    4 :

Consider this nasty thing:

tag: stag | etag ;
stag
    : "<" ~ "[^>]*>"
    ;
etag
    : "<" ~ "[^>]*/>"
    ;

In this case, the "~" operator is superfluous and will always return a
zero-length string if the entire rule matches.

*** Quoted String with embedded quotes
Several styles of quoted string we might like to handle.
a) what is the quote character?
b) can you embed literal quote character by doubling it?
c) can you embed literal quote character with an escape character?
d) do we handle "heredoc" syntaxes?
e) do we offer to create "compiled" string with escape sequences
   resolved?
f) 

